{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c792f3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:18:10.297743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "# sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6f47f",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d7175ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e353d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(\"../poetry_generator/data/robert_frost.txt\"):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    input_line = '<sos>' + line\n",
    "    target_line = line + '<eos>'\n",
    "    \n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e940084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>Two roads diverged in a yellow wood,',\n",
       " '<sos>And sorry I could not travel both',\n",
       " '<sos>And be one traveler, long I stood',\n",
       " '<sos>And looked down one as far as I could',\n",
       " '<sos>To where it bent in the undergrowth;',\n",
       " '<sos>Then took the other, as just as fair,',\n",
       " '<sos>And having perhaps the better claim',\n",
       " '<sos>Because it was grassy and wanted wear,',\n",
       " '<sos>Though as for that the passing there',\n",
       " '<sos>Had worn them really about the same,',\n",
       " '<sos>And both that morning equally lay',\n",
       " '<sos>In leaves no step had trodden black.',\n",
       " '<sos>Oh, I kept the first for another day!',\n",
       " '<sos>Yet knowing how way leads on to way',\n",
       " '<sos>I doubted if I should ever come back.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = input_texts + target_texts\n",
    "all_lines[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827e66e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866d72a",
   "metadata": {},
   "source": [
    "### Converting sentences into integers\n",
    "#### Tokenizer 1) Splits into individual tokens/words 2) each word is converted into an integer index for mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0633cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7718ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[133, 571, 572, 7, 3, 573],\n",
       " [4, 574, 5, 66, 28, 984],\n",
       " [4, 24, 25, 985, 163, 5],\n",
       " [4, 181, 69, 25, 14, 137, 14, 5],\n",
       " [2, 39, 8, 986, 7, 1],\n",
       " [130, 197, 1, 575, 14, 73, 14],\n",
       " [4, 145, 305, 1, 260],\n",
       " [440, 8, 12, 576, 4, 244],\n",
       " [155, 14, 13, 11, 1, 577, 2280],\n",
       " [21, 579, 50, 415, 122, 1],\n",
       " [4, 153, 11, 987, 988, 2281],\n",
       " [7, 989, 44, 581, 21, 990, 2282],\n",
       " [941, 5, 261, 1, 262, 13, 323],\n",
       " [191, 992, 106, 80, 993, 15, 2],\n",
       " [5, 994, 29, 5, 126, 154, 86, 2283]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "target_sequences[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa911183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[413, 571, 572, 7, 3, 573, 808], [16, 574, 5, 66, 28, 984, 153], [16, 24, 25, 985, 163, 5, 258], [16, 181, 69, 25, 14, 137, 14, 5, 66], [59, 39, 8, 986, 7, 1, 2408], [259, 197, 1, 575, 14, 73, 14, 2409], [16, 145, 305, 1, 260, 2410], [2411, 8, 12, 576, 4, 244, 2412], [414, 14, 13, 11, 1, 577, 58], [578, 579, 50, 415, 122, 1, 809], [16, 153, 11, 987, 988, 580], [102, 989, 44, 581, 21, 990, 991], [2413, 5, 261, 1, 262, 13, 323, 2414], [810, 992, 106, 80, 993, 15, 2, 80], [30, 994, 29, 5, 126, 154, 86, 324]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(input_sequences[:15])\n",
    "\n",
    "max_seq_len_from_data = max(len(s) for s in input_sequences)\n",
    "max_seq_len_from_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d09c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0047e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
