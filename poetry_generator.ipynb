{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1c377b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 11:18:10.297743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam, SGD\n",
    "# sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47845eef",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789c5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2966371",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open(\"../poetry_generator/data/robert_frost.txt\"):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    input_line = '<sos>' + line\n",
    "    target_line = line + '<eos>'\n",
    "    \n",
    "    input_texts.append(input_line)\n",
    "    target_texts.append(target_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806767da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>Two roads diverged in a yellow wood,',\n",
       " '<sos>And sorry I could not travel both',\n",
       " '<sos>And be one traveler, long I stood',\n",
       " '<sos>And looked down one as far as I could',\n",
       " '<sos>To where it bent in the undergrowth;',\n",
       " '<sos>Then took the other, as just as fair,',\n",
       " '<sos>And having perhaps the better claim',\n",
       " '<sos>Because it was grassy and wanted wear,',\n",
       " '<sos>Though as for that the passing there',\n",
       " '<sos>Had worn them really about the same,',\n",
       " '<sos>And both that morning equally lay',\n",
       " '<sos>In leaves no step had trodden black.',\n",
       " '<sos>Oh, I kept the first for another day!',\n",
       " '<sos>Yet knowing how way leads on to way',\n",
       " '<sos>I doubted if I should ever come back.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines = input_texts + target_texts\n",
    "all_lines[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ed547d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2872"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85102715",
   "metadata": {},
   "source": [
    "### Converting sentences into integers and word to integer mapping\n",
    "#### Tokenizer 1) Splits into individual tokens/words 2) each word is converted into an integer index for mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9172e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbfaa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[133, 571, 572, 7, 3, 573],\n",
       " [4, 574, 5, 66, 28, 984],\n",
       " [4, 24, 25, 985, 163, 5],\n",
       " [4, 181, 69, 25, 14, 137, 14, 5],\n",
       " [2, 39, 8, 986, 7, 1],\n",
       " [130, 197, 1, 575, 14, 73, 14],\n",
       " [4, 145, 305, 1, 260],\n",
       " [440, 8, 12, 576, 4, 244],\n",
       " [155, 14, 13, 11, 1, 577, 2280],\n",
       " [21, 579, 50, 415, 122, 1],\n",
       " [4, 153, 11, 987, 988, 2281],\n",
       " [7, 989, 44, 581, 21, 990, 2282],\n",
       " [941, 5, 261, 1, 262, 13, 323],\n",
       " [191, 992, 106, 80, 993, 15, 2],\n",
       " [5, 994, 29, 5, 126, 154, 86, 2283]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
    "tokenizer.fit_on_texts(all_lines)\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "target_sequences[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc64494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[413, 571, 572, 7, 3, 573, 808], [16, 574, 5, 66, 28, 984, 153], [16, 24, 25, 985, 163, 5, 258], [16, 181, 69, 25, 14, 137, 14, 5, 66], [59, 39, 8, 986, 7, 1, 2408], [259, 197, 1, 575, 14, 73, 14, 2409], [16, 145, 305, 1, 260, 2410], [2411, 8, 12, 576, 4, 244, 2412], [414, 14, 13, 11, 1, 577, 58], [578, 579, 50, 415, 122, 1, 809], [16, 153, 11, 987, 988, 580], [102, 989, 44, 581, 21, 990, 991], [2413, 5, 261, 1, 262, 13, 323, 2414], [810, 992, 106, 80, 993, 15, 2, 80], [30, 994, 29, 5, 126, 154, 86, 324]]\n",
      "Maximum sequence length 11\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences[:15])\n",
    "\n",
    "max_seq_len_from_data = max(len(s) for s in input_sequences)\n",
    "print('Maximum sequence length', max_seq_len_from_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014c40f",
   "metadata": {},
   "source": [
    "#### word to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f70ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens = 4614\n"
     ]
    }
   ],
   "source": [
    "word_to_index = tokenizer.word_index\n",
    "print('unique tokens =',len(word_to_index))\n",
    "# assert('<sos>' in word_to_index)\n",
    "# assert('<eos>' in word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a06d3",
   "metadata": {},
   "source": [
    "### Padding sequences --> by adding padding tokens, all sequences of varying lengths have same length so that they can be effectively processed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92465b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = min(max_seq_len_from_data, MAX_SEQUENCE_LENGTH)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='post')\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "524a3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence is [413 571 572   7   3 573 808   0   0   0   0] with the size of 11\n",
      "Target sequence is [133 571 572   7   3 573   0   0   0   0   0] with the size of 11\n"
     ]
    }
   ],
   "source": [
    "print('Input sequence is {} with the size of {}' .format(input_sequences[0], input_sequences.shape[1]))\n",
    "print('Target sequence is {} with the size of {}' .format(target_sequences[0], target_sequences.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72841f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8402df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f641e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e19733a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3d768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
